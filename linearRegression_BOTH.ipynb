{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous assignment you implemented the cost function and one step of gradient descent. Now it's time to put your code together and implement to full gradient descent algorithm. Please reuse the implementations of your functions from the previous assignment, but make sure they are correct. \n",
    "\n",
    "Make sure to comment your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a prediction function h\n",
    "def prediction(x,theta0,theta1):\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the output of that function to compute the cost function J:\n",
    "def cost(x, y,theta0,theta1):\n",
    "    return 1/len(x) * sum((h(x,theta0,theta1) - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function that returns the gradient values, given h (x_predict), y and x:\n",
    "def gradDescentStep(x_predict,y, x):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat the gradient descent step untill it converges.\n",
    "# These are some default parameters, see how playing with them affects the behavior\n",
    "alpha = 0.1\n",
    "theta0 = 0\n",
    "theta1 = 1\n",
    "iterations = 10\n",
    "\n",
    "#Fill in the stopcondition yourself\n",
    "stopcondition = 0\n",
    "\n",
    "i = 0\n",
    "cost = 10\n",
    "#Try to save the output of the cost function at each iteration and plot it at the end\n",
    "while (i < iterations) and (cost > stopcondition):\n",
    "    i = i +1\n",
    "    #Put everything together here\n",
    "    \n",
    "plt.plot(x,predict(x,theta0,theta1))\n",
    "plt.plot(x,y,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Write a short analysis about the amount of iterations necessary to obtain a good result, the influence of the learning rate and the trend of the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
